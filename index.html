<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="description" content="Your description goes here">
	<meta name="keywords" content="your,keywords,goes,here">
	<meta name="author" content="Your Name">
	<meta http-equiv="cache-control" content="no-cache, must-revalidate, post-check=0, pre-check=0">
  	<meta http-equiv="expires" content="Sat, 31 Oct 3016 00:00:00 GMT">
  	<meta http-equiv="pragma" content="no-cache">
	<link href="./Zhongqiu Wang_files/css" rel="stylesheet" type="text/css">
    <!--
	<link rel="stylesheet" type="text/css" href="./Zhongqiu Wang_files/origo.css" title="Zhong-Qiu Wang" media="all">
    -->
	<title>Zhong-Qiu Wang</title>
    <style>
    mark {
        background-color: red;
        color: white;
    }
    </style>
</head>

<body class="light blue freestyle01" data-feedly-mini="yes">
<div id="layout">
	<div class="row">
		<div class="col c6">
			<h1><font color="#000000">Zhong-Qiu Wang</font></h1>
			<hr color="#0ABAB5"; size="5">
		</div>
	</div>

	<div class="row">
        <div class="col c2">
            <img src="./Zhongqiu Wang_files/IMG_2238.jpg" alt="" width="137" height="180">
        </div>
        <div class="col c10">
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">  
            Hi, this is <strong>Zhong-Qiu Wang</strong> (in Chinese: <strong>王中秋</strong>) from <a href="https://en.wikipedia.org/wiki/Chongqing">Chongqing, China</a>, a 3D city renowned for its magical landscape, spicy food, and rap music.
            <br>
            I am currently a Tenure-Track Associate Professor in the <a href="https://cse.sustech.edu.cn/en/" target="view_window">Department of Computer Science and Engineering</a> at <a href="https://www.sustech.edu.cn/en/" target="view_window">Southern University of Science and Technology (SUSTech)</a> in <a href="https://en.wikipedia.org/wiki/Shenzhen" target="view_window">Shenzhen, China</a>.
            <br>
            I am the director of the <a href="https://sustechailab.github.io/" target="view_window">SUSTech Audio Intelligence Lab</a>.
            <br>
            My research interest includes (but not limited to):
            <br>
            &#8226; Speech and audio signal processing
                <br>
                &emsp; &#9675; Speech separation (e.g., speaker separation, speech enhancement, and speech dereverberation)
                <br>
                &emsp; &#9675; Sound understanding (e.g., sound event detection, audio tagging, and sound separation)
                <br>
                &emsp; &#9675; Assistive hearing (e.g., hearing aids design and smart hearables)
                <br>
                &emsp; &#9675; Real-time speech communication (e.g., acoustic echo cancellation, speech codec, and bandwidth extension)
                <br>
                &emsp; &#9675; Robust automatic speech recognition (e.g., noise-robust and multi-speaker ASR)
                <br>
                &emsp; &#9675; Microphone array signal processing (e.g., acoustic beamforming and sound source localization)
                <br>
                &emsp; &#9675; Computer audition
                <br>
            &#8226; Deep learning
            <br>
            &#8226; Artificial intelligence

            <p class="STYLE1">Email: wang.zhongqiu41 AT gmail.com / wangzq3 AT sustech.edu.cn</p>
            <p class="STYLE1">Office: Room 517, South Tower, Engineering Building (工学院南楼517)</p>
            [<a href="./cv/Zhong-Qiu_Wang_cv.pdf" target="view_window">CV</a>]
            [<a href="https://scholar.google.com/citations?user=fGUzTN8AAAAJ&hl=en" target="view_window">Google Scholar</a>] 
            [<a href="https://www.linkedin.com/in/zhong-qiu-wang/" target="view_window">LinkedIn</a>]
            <!--
            [<a href="index_chinese.html" target="view_window">Chinese Version</a>]
            -->

            <!--
            -->
            <br>
            <br>
            &#8226; <font style="font-size: 14pt" font color="red"><strong>
            Our group has openings for Master students (保研/考研), Ph.D. students, research associates (研究助理) and visiting students (访问学生). If you are interested in joining us, please send Prof. Wang your CV, or visit Prof. Wang's office in person.
            </strong></font>
            <!--
            <br>
            &#8226; 招收2名南科大校内或校外保研学生（2026年秋季入学）
            -->
            <br>
            &#8226; 招收1名考研学生（学硕，2026年秋季入学）和2名考研学生（工程硕士，2026年秋季入学）
            <!--
            <br>
            &#8226; 招收1名<a href="perspective_students/2026_university_and_major_list.pdf" target="view_window">如下列表学校和专业</a>中的保研学生（2026年秋季入学，属奖励名额）
            <br>
            &#8226; 招收1名<a href="https://cse.sustech.edu.cn/cn/news/view/id/1112" target="view_window">国优计划</a>保研学生（2026年秋季入学），从事语音处理在教育中的研究与应用开发
            -->
            <br>
            &#8226; 招收1名博士学生（<a href="https://cse.sustech.edu.cn/cn/news/view/id/1116" target="view_window">快响计划</a>，2026年秋季入学）
            <br>
            &#8226; 招收1名研究助理（长期有效）
            <br>

            </font></p>
        </div>	
	</div>

	<div class="row">
		<div class="col c12">

			<h2><font color="#808080">Employment</font></h2>
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            &#8226; Tenure-Track Associate Professor, <a href="https://cse.sustech.edu.cn/en/" target="view_window">Department of Computer Science and Engineering</a> at <a href="https://www.sustech.edu.cn/en/" target="view_window">Southern University of Science and Technology</a>, Jul. 2024 ~ present.

            <br>

            &#8226; Postdoctoral Research Associate, <a href="https://lti.cs.cmu.edu/">Language Technologies Institute</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, Sep. 2021 ~ Jul. 2024.

            <br>

            &#8226; Visiting Research Scientist, <a href="https://www.merl.com/research/speech-audio">Speech and Audio Group</a> at <a href="http://www.merl.com/">Mitsubishi Electric Research Laboratories</a>, Jun. 2020 ~ Aug. 2021.

            <br>

			&#8226; Research Intern, Sound Understanding Team at <a href="https://ai.google/research/teams/perception/">Google AI Perception</a>, May 2019 ~ Aug. 2019.

			<br>

			&#8226; Research Intern, <a href="https://www.merl.com/research/speech-audio">Speech and Audio Group</a> at <a href="http://www.merl.com/">Mitsubishi Electric Research Laboratories</a>, May 2017 ~ Aug. 2017.

			<br>

			&#8226; Research Intern, <a href="https://www.microsoft.com/en-us/research/group/audio-and-acoustics-research-group/">Audio and Acoustics Research Group</a> at <a href="http://research.microsoft.com/en-us/">Microsoft Research Redmond</a>, May 2016 ~ Aug. 2016.
			</font></p>


			<h2><font color="#808080">Education</font></h2>
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">  
			&#8226; Ph.D., <a href="https://cse.osu.edu">Department of Computer Science and Engineering</a> at <a href="https://www.osu.edu">The Ohio State University</a>, USA, Aug. 2013 ~ May 2020.
			<br>
			&#8226; M.Sc., <a href="https://cse.osu.edu">Department of Computer Science and Engineering</a> at <a href="https://www.osu.edu">The Ohio State University</a>, USA, Aug. 2013 ~ Aug. 2017.
			<br>
			&#8226; B.Eng., <a href="http://cs.hit.edu.cn/">School of Computer Science and Technology</a> at <a href="https://en.wikipedia.org/wiki/Harbin_Institute_of_Technology">Harbin Institute of Technology</a>, China, Aug. 2009 ~ Jul. 2013.
			</font></p>

			<h2><font color="#808080">Journal Publications</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [23] S. Cornell, C. Boeddeker, T. Park, H. Huang, D. Raj, M. Wiesner, Y. Masuyama, X. Chang, <strong>Z.-Q. Wang</strong>, S. Squartini, P. Garcia, and S. Watanabe, "<a href="https://arxiv.org/abs/2507.18161" target="view_window">Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges</a>", in <i>Computer Speech & Language (<strong>CSL</strong>)</i>, vol. 97, issue 101901, pp. 1-36, 2026.

            <br>

            [22] Y. Masuyama, X. Chang, W. Zhang, S. Cornell, <strong>Z.-Q. Wang</strong>, N. Ono,Y. Qian, and S. Watanabe, "<a href="publications/Yoshiki_CSL_2025.pdf", target="view_window">An End-to-End Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</a>", in <i><strong>CSL</strong></i>, vol. 95, issue 101813, pp. 1-18, 2026.

            <br>

            [21] P. Shen, K. Chen, S. He, P. Chen, S. Yuan, H. Kong, X. Zhang, and <strong>Z.-Q. Wang</strong>, "<a href="publications/Listen_to_Extract_Onset-Prompted_Target_Speaker_Extraction.pdf" target="view_window">Listen to Extract: Onset-Prompted Target Speaker Extraction</a>", in <i>IEEE Transactions on Audio, Speech and Language Processing (<strong>TASLPRO</strong>)</i>, vol. 33, pp. 4832-4843, 2025. [<a href="./demos/LExt_demo/index.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [20] <u><strong>Z.-Q. Wang</strong></u>, "<a href="publications/JASA_ctPulSE.pdf" target="view_window">ctPuLSE: Close-Talk, and Pseudo-Label Based Far-Field, Speech Enhancement</a>", in <i>Journal of The Acoustical Society of America (<strong>JASA</strong>)</i>, vol. 158, issue 4, pp. 2849-2862, 2025. [<a href="./demos/SuperME_demo/index.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [19] <strong>Z.-Q. Wang</strong>, "<a href="publications/SuperM2M.pdf" target="view_window">SuperM2M: Supervised and Mixture-to-Mixture Co-Learning for Speech Enhancement and Noise-Robust ASR</a>", in <i>Neural Networks (<strong>NN</strong>)</i>, vol. 188, issue 107408, pp. 1-16, 2025. [<a href="./demos/SuperME_demo/index.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [18] <strong>Z.-Q. Wang</strong>, "<a href="publications/TASLP2024_USDnet_Unsupervised_Speech_Dereverberation_via_Neural_Forward_Filtering.pdf" target="view_window">USDnet: Unsupervised Speech Dereverberation via Neural Forward Filtering</a>", in <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<strong>IEEE/ACM TASLP</strong>)</i>, vol. 32, pp. 3882-3895, 2024. [<a href="./demos/USDnet_demo/index.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [17] <strong>Z.-Q. Wang</strong>, "<a href="publications/Mixture_to_Mixture_Leveraging_Close-Talk_Mixtures_as_Weak-Supervision_for_Speech_Separation.pdf" target="view_window">Mixture to Mixture: Leveraging Close-talk Mixtures as Weak-supervision for Speech Separation</a>", in <i>IEEE Signal Processing Letters (<strong>IEEE SPL</strong>)</i>, vol. 31, pp. 1715-1719, 2024. [<a href="./demos/M2M_demo/index.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [16] Y.-J. Lu, X. Chang, C. Li, W. Zhang, S. Cornell, Z. Ni, Y. Masuyama, B. Yan, R. Scheibler, <strong>Z.-Q. Wang</strong>, Y. Tsao, Y. Qian, and S. Watanabe, "<a href="https://joss.theoj.org/papers/10.21105/joss.05403" target="view_window">Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing</a>", in <i>Journal of Open Source Software (<strong>JOSS</strong>)</i>, vol. 8, iss. 91, 5403, 2023.

            <br>

            [15] <strong>Z.-Q. Wang</strong>, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, "<a href="publications/TASLP2023_TF-GridNet.pdf" target="view_window">TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 31, pp. 3221-3236, 2023. [<a href="./demos/TF-GridNet-demo/index.html" target="view_window"><u>Sound Demo</u></a>] [<a href="https://github.com/espnet/espnet/pull/5395" target="view_window"><u>Code</u></a>]

            <br>

            [14] D. Petermann, G. Wichern, A. Subramanian, <strong>Z.-Q. Wang</strong>, and J. Le Roux, "<a href="publications/TASLP2023_Tackling_the_Cocktail_Fork_Problem_for_Separation_and_Transcription_of_Real-World_Soundtracks.pdf" target="view_window">Tackling The Cocktail Fork Problem for Separation and Transcription of Real-World Soundtracks</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 31, pp. 2592-2605, 2023.

            <br>

            [13] <strong>Z.-Q. Wang</strong>, G. Wichern, S. Watanabe, and J. Le Roux, "<a href="publications/TASLP2022_STFTlowlat.pdf" target="view_window">STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 31, pp. 397-410, 2022.

            <br>

            [12] <strong>Z.-Q. Wang</strong> and S. Watanabe, "<a href="publications/spl2022_ofp.pdf" target="view_window">Improving Frame-Online Neural Speech Enhancement with Overlapped-Frame Prediction</a>", in <i><strong>IEEE SPL</strong></i>, vol. 29, pp. 1422-1426, 2022.

            <br>

            [11] K. Tan, <strong>Z.-Q. Wang</strong>, and D.L. Wang, "<a href="publications/TWW.taslp22.pdf" target="view_window">Neural Spectrospatial Filtering</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 30, pp. 605-621, 2022.

            <br>

            [10] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="publications/taslp2021_fcp.pdf" target="view_window">Convolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 29, pp. 3476-3490, 2021.

            <br>

            [9] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="publications/spl2021_magnitude_phase_compensation.pdf" target="view_window">On The Compensation Between Magnitude and Phase in Speech Separation</a>", in <i><strong>IEEE SPL</strong></i>, vol. 28, pp. 2018-2022, 2021.

            <br>

            [8] <strong>Z.-Q. Wang</strong>, P. Wang, and D.L. Wang, "<a href="publications/WWW.taslp21.pdf" target="view_window">Multi-Microphone Complex Spectral Mapping for Utterance-Wise and Continuous Speech Separation</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 29, pp. 2001-2014, 2021. [<a href="./demos/SMSWSJ_demo/taslp20_SMSWSJ_demo.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [7] <strong>Z.-Q. Wang</strong>*, P. Wang*, and D.L. Wang, "<a href="publications/TASLP2020_CHiME-4.pdf">Complex Spectral Mapping for Single- and Multi-Channel Speech Enhancement and Robust ASR</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 28, pp. 1778-1787, 2020. [* denotes equal contribution, <a href="https://web.cse.ohio-state.edu/~wang.7642/homepage/demos/taslp19_chime4_demo.html" target="view_window"><u>Sound Demo</u></a>]

            <br>

            [6] H. Taherian, <strong>Z.-Q. Wang</strong>, J. Chang, and D.L. Wang, "<a href="publications/TWCW.taslp20.pdf">Robust Speaker Recognition Based on Single-Channel and Multi-Channel Speech Enhancement</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 28, pp. 1293-1302, 2020.

            <br>

            [5] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/Wang-Wang.taslp20.pdf">Deep Learning Based Target Cancellation for Speech Dereverberation</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 28, pp. 941-950, 2020. <a href="code/TASLP2019_DEREVERB_create-spatialized-mixtures.zip" target=""><u>Data Generation Code</u></a>

            <br>

            [4] Y. Zhao, <strong>Z.-Q. Wang</strong>, and D.L. Wang, "<a href="publications/ZWW.taslp19.pdf" target="view_window">Two-Stage Deep Learning for Noisy-Reverberant Speech Enhancement</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 27, pp. 53-62, 2019.

            <br>

            [3] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/Wang-Wang.taslp19.pdf" target="view_window">Combining Spectral and Spatial Features for Deep Learning Based Blind Speaker Separation</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 27, pp. 457-468, 2019.

            <br>

            [2] <strong>Z.-Q. Wang</strong>, X. Zhang, and D.L. Wang, "<a href="publications/WZW.taslp19.pdf" target="view_window">Robust Speaker Localization Guided by Deep Learning Based Time-Frequency Masking</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 27, pp. 178-188, 2019.

            <br>

            [1] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/taslp2016.pdf", target="view_window">A Joint Training Framework for Robust Automatic Speech Recognition</a>", in <i><strong>IEEE/ACM TASLP</strong></i>, vol. 24, pp. 796-806, 2016.

            </font></p>

			<h2><font color="#808080">Conference Publications in ML/AI</font></h2> 
			<p class="style1" align="justify"><font style="font-size: 12pt">

            [3] Z. Xu, X. Fu, <strong>Z.-Q. Wang</strong>, X. Jiang, and R. Roy Choudhury, "<a href="https://arxiv.org/pdf/2505.05657" target="view_window">Unsupervised Blind Speech Separation with A Diffusion Prior</a>", in <i>International Conference on Machine Learning (<strong>ICML</strong>)</i>, 2025. [<a href="https://arraydps.github.io/ArrayDPSDemo/" target="view_window"><u>Sound Demo</u></a>] [<a href="https://github.com/ArrayDPS/ArrayDPS" target="view_window"><u>Code</u></a>]

            <br>

            [2] <strong>Z.-Q. Wang</strong>, A. Kumar, and S. Watanabe, "<a href="https://www.ijcai.org/proceedings/2024/0572.pdf" target="view_window">Cross-Talk Reduction</a>", in <i>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</i>, pp. 5171-5180, 2024. [<a href="./demos/CTRnet_demo/index.html" target="view_window"><u>Sound Demo</u></a>] [<a href="./publications/ijcai2024_poster.pdf" target="view_window"><u>Poster</u></a>] [<a href="./publications/ijcai2024_slide.pdf" target="view_window"><u>Slide</u></a>]

            <br>

            [1] <strong>Z.-Q. Wang</strong> and S. Watanabe, "<a href="https://papers.nips.cc/paper_files/paper/2023/file/6b44765c9201730a27f7931afb4d7434-Paper-Conference.pdf" target="view_window">UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures</a>", in <i>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, pp. 34021-34042, 2023. [<a href="./demos/UNSSOR-demo/index.html" target="view_window"><u>Sound Demo</u></a>] [<a href="./publications/NeurIPS2023_UNSSOR_poster.pdf" target="view_window"><u>Poster</u></a>]
            
            </font></p>

			<h2><font color="#808080">Conference Publications in Speech/Audio</font></h2> 
			<p class="style1" align="justify"><font style="font-size: 12pt">

			<!--
            -->

            [57] <strong>Z.-Q. Wang</strong> and <u>R. Pang</u>, "<a href="https://arxiv.org/abs/2507.15229" target="view_window">Mixture to Beamformed Mixture: Leveraging Beamformed Mixture as Weak-Supervision for Speech Enhancement and Noise-Robust ASR</a>", in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>)</i>, 2026.

            <br>

            [56] S. He and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2510.08914" target="view_window">VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR Virtual Microphone Arrays</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [55] J. Sun, S. He, R. Pang, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2510.05757" target="view_window">Neural Forward Filtering for Speaker-Image Separation</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [54] P. Shen, S. He, X. Zhang, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2510.24471" target="view_window">LExTra: Folded Prompt and Split-Role Attention for Target Speaker Extraction</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [53] T. Ling, S. He, P. Shen, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2510.15437" target="view_window">MC-LExt: Multi-Channel Target Speaker Extraction with Onset-Prompted Speaker Conditioning Mechanism</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [52] Y. Zhu, J. Jin, X. Luo, W. Yang, <strong>Z.-Q. Wang</strong>, G. Huang, J. Chen, and J. Benesty, "<a href="https://arxiv.org/abs/2510.24471" target="view_window">Forward Convolutive Prediction for Frame Online Monaural Speech Dereverberation Based on Kronecker Product Decomposition</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [51] P. Lu, P. Zhou, X. Chen, J. Wang, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2507.15229" target="view_window">UJCodec: An End-to-End UNet-Style Codec for Joint Speech Compression and Enhancement</a>", in <i><strong>ICASSP</strong></i>, 2026.

            <br>

            [50] R. Sachdev, <strong>Z.-Q. Wang</strong>, and C.-H. H. Yang, "<a href="https://arxiv.org/abs/2407.16370" target="view_window">Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction</a>", in <i>IEEE Workshop on Signal Processing Systems (<strong>SiPS</strong>)</i>, pp. 131-135, 2025.

            <br>

            [49] Y. Wu, Z. Xu, J. Chen, <strong>Z.-Q. Wang</strong>, and R. Roy Choudhury, "<a href="https://arxiv.org/abs/2508.02071" target="view_window">Unsupervised Multi-Channel Speech Dereverberation via Diffusion</a>", in <i>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (<strong>WASPAA</strong>)</i>, 2025.
 
            <br>

            [48] P. Shen, X. Zhang, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/pdf/2505.22051" target="view_window">ARiSE: Auto-Regressive Multi-Channel Speech Enhancement</a>", in <i>Annual Conference of the International Speech Communication Association (<strong>Interspeech</strong>)</i>, pp. 1183-1187, 2025.

            <br>

            [47] F. Zhao, X. Zhang, and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2505.19493" target="view_window">Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation</a>", in <i><strong>Interspeech</strong></i>, pp. 629-633, 2025.

            <br>

            [46] L. Fu, Y. Liu, Z. Liu, Z. Yang, <strong>Z.-Q. Wang</strong>, Y. Li, and H. Kong, "<a href="">AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers</a>", in <i><strong>Interspeech</strong></i>, pp. 938-942, 2025.

            <br>

            [45] S. Araki, N. Ito, R. Haeb-Umbach, G. Wichern, <strong>Z.-Q. Wang</strong>, and Y. Mitsufuji, "<a href="https://arxiv.org/pdf/2501.11837" target="view_window">30+ Years of Source Separation Research: Achievements and Future Challenges</a>", in <i><strong>ICASSP</strong></i>, 2025.

            <br>

            [44] H. Chen, S. Wu, C. Wang, J. Du, C.-H. Lee, S. Siniscalchi, S. Watanabe, J. Chen, O. Scharenborg, <strong>Z.-Q. Wang</strong>, B.-C. Yin, and J. Pan, "<a href="" target="view_window">Summary on The Multimodal Information-Based Speech Processing (MISP) 2023 Challenge</a>", in <i><strong>ICASSP Workshop</strong></i>, pp. 123-124, 2024.

            <br>

            [43] S. Wu, C. Wang, H. Chen, Y. Dai, C. Zhang, R. Wang, H. Lan, J. Du, C.-H. Lee, J. Chen, S. Watanabe, S. Siniscalchi, O. Scharenborg, <strong>Z.-Q. Wang</strong>, J. Pan, and J. Gao, "<a href="https://arxiv.org/abs/2309.08348" target="view_window">The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction</a>", in <i><strong>ICASSP</strong></i>, pp. 8351-8355, 2024.
			<!-- -->

            <br>

            [42] Y. Lee, S. Choi, B.-Y. Kim, <strong>Z.-Q. Wang</strong>, and S. Watanabe, "<a href="https://arxiv.org/abs/2401.12473" target="view_window">Boosting Unknown-Number Speaker Separation with Transformer Decoder-based Attractor</a>", in <i><strong>ICASSP</strong></i>, pp. 446-450, 2024.

            <br>

            [41] K. Saijo, W. Zhang, <strong>Z.-Q. Wang</strong>, S. Watanabe, T. Kobayashi, and T. Ogawa, "<a href="https://arxiv.org/abs/2310.08277" target="view_window">A Single Speech Enhancement Model Unifying Dereverberation, Denoising, Speaker Counting, Separation, and Extraction</a>", in <i>IEEE Automatic Speech Recognition and Understanding Workshop (<strong>ASRU</strong>)</i>, 2023.

            <br>

            [40] W. Zhang, K. Saijo, <strong>Z.-Q. Wang</strong>, S. Watanabe, and Y. Qian, "<a href="https://arxiv.org/abs/2309.17384" target="view_window">Toward Universal Speech Enhancement for Diverse Input Conditions</a>", in <i><strong>ASRU</strong></i>, 2023.

            <br>

            [39] S. Cornell, M. Wiesner, S. Watanabe, D. Raj, X. Chang, P. Garcia, Y. Masuyama, <strong>Z.-Q. Wang</strong>, S. Squartini, and S. Khudanpur, "<a href="https://arxiv.org/abs/2306.13734" target="view_window">The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios</a>", in <i>Proceedings of CHiME Challenge</i>, 2023.

            <br>

            [38] Y. Masuyama, X. Chang, W. Zhang, S. Cornell, <strong>Z.-Q. Wang</strong>, N. Ono, Y. Qian, and S. Watanabe, "<a href="https://arxiv.org/pdf/2307.12231.pdf" target="view_window">Exploring The Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</a>", in <i><strong>WASPAA</strong></i>, 2023.

            <br>

            [37] <strong>Z.-Q. Wang</strong>, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, "<a href="publications/TF-GRIDNET_Making_Time-Frequency_Domain_Models_Great_Again_for_Monaural_Speaker_Separation.pdf" target="view_window">TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation</a>", in <i><strong>ICASSP</strong></i>, 2023.
            
            <br>

            [36] <strong>Z.-Q. Wang</strong>, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, "<a href="publications/Neural_Speech_Enhancement_with_Very_Low_Algorithmic_Latency_and_Complexity_via_Integrated_full-_and_sub-band_Modeling.pdf" target="view_window">Neural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated Full- and Sub-Band Modeling</a>", in <i><strong>ICASSP</strong></i>, 2023.

            <br>

            [35] S. Cornell, <strong>Z.-Q. Wang</strong>, Y. Masuyama, S. Watanabe, M. Pariente, N. Ono, and S. Squartini, "<a href="publications/Multi-Channel_Speaker_Extraction_with_Adversarial_Training_The_Wavlab_Submission_to_The_Clarity_ICASSP_2023_Grand_Challenge.pdf" target="view_window">Multi-Channel Speaker Extraction with Adversarial Training: The WAVlab Submission to The Clarity ICASSP 2023 Grand Challenge</a>", in <i><strong>ICASSP</strong></i>, 2023.

            <br>

            [34] S. Cornell, <strong>Z.-Q. Wang</strong>, Y. Masuyama, S. Watanabe, M. Pariente, and N. Ono, "<a href="https://claritychallenge.org/clarity2022-CEC2-workshop/papers/Clarity_2022_CEC2_paper_cornell.pdf" target="view_window">Multi-Channel Target speaker Extraction with Refinement: The WAVLab Submission to The Second Clarity Enhancement Challenge</a>", in <i>Proceedings of Clarity Challenge</i>, 2022. [<font color="red"><strong>Winner (1st/13 submissions) of The 2nd Clarity Enhancement Challenge</strong></font>, <a href="https://claritychallenge.org/docs/cec2/cec2_intro" color="red" target="view_window">challenge description</a>, <a href="https://claritychallenge.org/clarity2022-CEC2-workshop/results.html" color="red" target="view_window">workshop</a>, <a href="demos/WAVLab_CEC2_demo/index.html" color="red" target="view_window">demo</a>]

            <br>

            <!---->
            [33] S. Choi, Y. Lee, J. Park, H. Y. Kim, B.-Y. Kim, <strong>Z.-Q. Wang</strong>, and S. Watanabe, "<a href="http://www.apsipa.org/proceedings/2022/APSIPA%202022/WedPM2-2/1570831206.pdf" target="view_window">An Empirical Study of Training Mixture Generation Strategies on Speech Separation: Dynamic Mixing and Augmentation</a>", in <i>Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (<strong>APSIPA-ASC</strong>)</i>, pp. 1071-1076, 2022.

            <br>

            [32] Y.-J. Lu,  X. Chang, C. Li, W. Zhang, S. Cornell, Z. Ni, Y. Masuyama, B. Yan, R. Scheibler, <strong>Z.-Q. Wang</strong>, Y. Tsao, Y. Qian, and S. Watanabe, "<a href="https://arxiv.org/abs/2207.09514" target="view_window">ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding</a>", in <i><strong>Interspeech</strong></i>, pp. 5458-5462, 2022.

            <br>

            [31] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/Localization_based_Sequential_Grouping_for_Continuous_Speech_Separation.pdf" target="view_window">Localization Based Sequential Grouping for Continuous Speech Separation</a>", in <i><strong>ICASSP</strong></i>, pp. 281-285, 2022.

            <br>

            <!---->
            [30] Y.-J. Lu, <strong>Z.-Q. Wang</strong>, S. Watanabe, A. Richard, C. Yu, and Y. Tsao, "<a href="https://arxiv.org/pdf/2202.05256.pdf">Conditional Diffusion Probabilistic Model for Speech Enhancement</a>", in <i><strong>ICASSP</strong></i>, pp. 7402-7402, 2022.

            <br>

            [29] Y.-J. Lu, S. Cornell, X. Chang, W. Zhang, C. Li, Z. Ni, <strong>Z.-Q. Wang</strong>, and S. Watanabe, "<a href="https://arxiv.org/pdf/2202.12298.pdf" target="view_window">Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNet-SE Submission to The L3DAS22 Challenge</a>", in <i><strong>ICASSP</strong></i>, pp. 9201-9205, 2022. [<font color="red"><strong>Winner (1st/17 teams) of L3DAS22 Speech Enhancement Challenge</strong></font>, <a href="https://www.l3das.com/icassp2022/results.html" color="red">challenge rankings</a>] [<a href="https://github.com/espnet/espnet/blob/master/espnet2/enh/separator/ineube_separator.py" target="view_window"><u>Code</u></a>]

            <br>

            [28] D. Petermann, G. Wichern, <strong>Z.-Q. Wang</strong>, and J. Le Roux, "<a href="https://arxiv.org/pdf/2110.09958.pdf" target="view_window">The Cocktail Fork Problem: Three-Stem Audio Separation for Real-World Soundtracks</a>", in <i><strong>ICASSP</strong></i>, pp. 526-530, 2022.

            <br>

            [27] O. Slizovskaia, G. Wichern, <strong>Z.-Q. Wang</strong>, and J. Le Roux, "<a href="https://arxiv.org/abs/2203.04197">Locate This, Not That: Class-Conditioned Sound Event DOA Estimation</a>", in <i><strong>ICASSP</strong></i>, pp. 711-715, 2022.

            <br>

            [26] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="https://arxiv.org/abs/2108.07194" target="view_window">Convolutive Prediction for Reverberant Speech Separation</a>", in <i><strong>WASPAA</strong></i>, pp. 56-60, 2021.

            <br>

            [25] G. Wichern, A. Chakrabarty, <strong>Z.-Q. Wang</strong>, and J. Le Roux, "<a href="https://www.merl.com/publications/docs/TR2021-129.pdf">Anomalous Sound Detection using Attentive Neural Processes</a>", in <i><strong>WASPAA</strong></i>, pp. 186-190, 2021.

            <br>

            [24] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/Wang-Wang.icassp21.pdf" target="view_window">Count and Separate: Incorporating Speaker Counting for Continuous Speech Separation</a>", in <i><strong>ICASSP</strong></i>, pp. 11-15, 2021.

            <br>

            <!---->
            [23] <strong>Z.-Q. Wang</strong>, H. Erdogan, S. Wisdom, K. Wilson, D. Raj, S. Watanabe, Z. Chen, and J. R. Hershey, "<a href="https://arxiv.org/abs/1911.07953" target="view_window">Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement</a>", in <i>IEEE Spoken Language Technology Workshop (<strong>SLT</strong>)</i>, pp. 905-911, 2021.

            <br>

            [22] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/Multi-Microphone_Complex_Spectral_Mapping_for_Speech_Dereverberation.pdf" target="view_window">Multi-Microphone Complex Spectral Mapping for Speech Dereverberation</a>", in <i><strong>ICASSP</strong></i>, pp. 486-490, 2020.

            <br>

            [21] H. Taherian, <strong>Z.-Q. Wang</strong>, and D.L. Wang, "<a href="https://www.isca-archive.org/interspeech_2019/taherian19_interspeech.html" target="view_window">Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments</a>", in <i><strong>Interspeech</strong></i>, pp. 4070-4074, 2019.

            <br>

            [20] <strong>Z.-Q. Wang</strong>, K. Tan, and D.L. Wang, "<a href="publications/Deep_Learning_Based_Phase_Reconstruction_for_Speaker_Separation_A_Trigonometric_Perspective.pdf" target="view_window">Deep Learning Based Phase Reconstruction for Speaker Separation: A Trigonometric Perspective</a>", in <i><strong>ICASSP</strong></i>, pp. 71-75, 2019.

            <br>

            [19] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="https://www.isca-archive.org/interspeech_2018/wang18k_interspeech.html" target="view_window">Integrating Spectral and Spatial Features for Multi-Channel Speaker Separation</a>", in <i><strong>Interspeech</strong></i>, pp. 2718-2722, 2018.

            <br>

            [18] <strong>Z.-Q. Wang</strong>, X. Zhang, and D.L. Wang, "<a href="https://www.isca-archive.org/interspeech_2018/wang18g_interspeech.html" target="view_window">Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks</a>", in <i><strong>Interspeech</strong></i>, pp. 322-326, 2018.

            <br>

            [17] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="https://www.isca-archive.org/interspeech_2018/wang18h_interspeech.html" target="view_window">All-Neural Multi-Channel Speech Enhancement</a>", in <i><strong>Interspeech</strong></i>, pp. 3234-3238, 2018.

            <br>

            [16] <strong>Z.-Q. Wang</strong>, J. Le Roux, D.L. Wang, and J. R. Hershey, "<a href="https://www.isca-archive.org/interspeech_2018/wang18f_interspeech.html" target="view_window">End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction</a>", in <i><strong>Interspeech</strong></i>, pp. 2708-2712, 2018.

            <br>

            [15] <strong>Z.-Q. Wang</strong>, J. Le Roux, and J. R. Hershey, "<a href="publications/Multi-Channel_Deep_Clustering_Discriminative_Spectral_and_Spatial_Embeddings_for_Speaker-Independent_Speech_Separation.pdf" target="view_window">Multi-Channel Deep Clustering: Discriminative Spectral and Spatial Embeddings for Speaker-Independent Speech Separation</a>", in <i><strong>ICASSP</strong></i>, pp. 1-5, 2018. [<font color="red"><strong>Best Student Paper Award</strong></font>]

            <br>

            [14] <strong>Z.-Q. Wang</strong>, J. Le Roux, and J. R. Hershey, "<a href="https://www.merl.com/publications/docs/TR2018-005.pdf" target="view_window">Alternative Objective Functions for Deep Clustering</a>", in <i><strong>ICASSP</strong></i>, pp. 686-690, 2018.

            <br>

            [13] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="publications/ZWang-Wang2.icassp18.pdf" target="view_window">On Spatial Features for Supervised Speech Separation and its Application to Beamforming and Robust ASR</a>", in <i><strong>ICASSP</strong></i>, pp. 5709-5713, 2018.

            <br>

            [12] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="./publications/icassp2018_1.pdf" target="view_window">Mask Weighted STFT Ratios for Relative Transfer Function Estimation and its Application to Robust ASR</a>", in <i><strong>ICASSP</strong></i>, pp. 5619-5623, 2018.

            <br>

            [11] I. Tashev, <strong>Z.-Q. Wang</strong>, and K. Godin, "<a href="http://ieeexplore.ieee.org/document/8023477/">Speech Emotion Recognition Based on Gaussian Mixture Models and Deep Neural Networks</a>", in <i>Information Theory and Applications Workshop (<strong>ITA</strong>)</i>, pp. 1-4, 2017.

            <br>

            [10] Y. Zhao, <strong>Z.-Q. Wang</strong>, and D.L. Wang, "<a href="./publications/icassp2017_5.pdf" target="view_window">A Two-stage Algorithm for Noisy and Reverberant Speech Enhancement</a>", in <i><strong>ICASSP</strong></i>, pp. 5580-5584, 2017.

            <br>

            [9] X. Zhang, <strong>Z.-Q. Wang</strong>, and D.L. Wang, "<a href="./publications/icassp2017_4.pdf" target="view_window">A Speech Enhancement Algorithm by Iterating Single- and Multi-microphone Processing and its Application to Robust ASR</a>", in <i><strong>ICASSP</strong></i>, pp. 276-280, 2017.

            <br>

            [8] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="./publications/icassp2017_1.pdf" target="view_window">Recurrent Deep Stacking Networks for Supervised Speech Separation</a>", in <i><strong>ICASSP</strong></i>, pp. 71-75, 2017.

            <br>

            [7] <strong>Z.-Q. Wang</strong> and I. Tashev, "<a href="./publications/icassp2017_3.pdf" , target="view_window">Learning Utterance-level Representations for Speech Emotion and Age/Gender Recognition using Deep Neural Networks</a>", in <i><strong>ICASSP</strong></i>, pp. 5150-5154, 2017.

            <br>

            [6] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="./publications/icassp2017_2.pdf", target="view_window">Unsupervised Speaker Adaptation of Batch Normalized Acoustic Models for Robust ASR</a>", in <i><strong>ICASSP</strong></i>, pp. 4890-4894, 2017.

            <br>

            [5] <strong>Z.-Q. Wang</strong>, Y. Zhao, and D.L. Wang, "<a href="./publications/icassp2016_1.pdf", target="view_window">Phoneme-Specific Speech Separation</a>", in <i><strong>ICASSP</strong></i>, pp. 146-150, 2016. <font color="red"><strong>[NSF Student Travel Grant]</strong></font>

            <br>

            [4] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="./publications/icassp2016_2.pdf" target="view_window">Robust Speech Recognition from Ratio Masks</a>", in <i><strong>ICASSP</strong></i>, pp. 5720-5724, 2016.

            <br>

            [3] D. Bagchi, M. Mandel, <u><strong>Z. Wang</strong></u>, Y. He, A. Plummer,, and E. Fosler-Lussier, "<a href="./publications/asru2015.pdf" target="view_window">Combining Spectral Feature Mapping and Multi-channel Model-based Source Separation for Noise-robust Automatic Speech Recognition</a>", in <i><strong>ASRU</strong></i>, pp. 496-503, 2015.

            <br>

            [2] <strong>Z.-Q. Wang</strong> and D.L. Wang, "<a href="https://www.isca-archive.org/interspeech_2015/wang15j_interspeech.html" target="view_window">Joint Training of Speech Separation, Filterbank and Acoustic Model for Robust Automatic Speech Recognition</a>", in <i><strong>Interspeech</strong></i>, pp. 2839-2843, 2015.

            <br>

            [1] Y. Liu, <u><strong>Z. Wang</strong></u>, M. Guo, and P. Li, "<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7025714" target="view_window">Hidden Conditional Random Field for Lung Nodule Detection</a>", in <i>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</i>, pp. 3518-3521, 2014.

            </font></p>

			<h2><font color="#808080">Patents</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [4] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="https://patents.google.com/patent/WO2023176704A1" target="view_window">Method and System for Audio Signal Enhancement with Reduced Latency</a>", US Patent Application 18/045,380, 2023.

            <br>

            [3] G. Wichern, A. Chakrabarty, <strong>Z.-Q. Wang</strong>, and J. Le Roux, "<a href="publications/US11978476_Method and System for Detecting Anomalous Sound.pdf" target="view_window">Method and System for Detecting Anomalous Sound</a>", US Patent 11,978,476 B2, 2024.

            <br>

            [2] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="publications/US11790930_Method and system for dereverberation of speech signals.pdf" target="view_window">Method and System for Dereverberation of Speech Signals</a>", US Patent 11,790,930 B2, 2023.

            <br>

            [1] J. Le Roux, J. R. Hershey, <u><strong>Z. Wang</strong></u>, and G. P. Wichern, "<a href="publications/US10529349_Methods and systems for end-to-end speech separation with unfolded iterative phase reconstruction.pdf" target="view_window">Methods and Systems for End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction</a>", US Patent 10,529,349 B2, 2020.

            </font></p>

			<h2><font color="#808080">Technical Reports</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [1] F. Wu and <strong>Z.-Q. Wang</strong>, "<a href="https://dcase.community/documents/challenge2025/technical_reports/DCASE2025_Wu_45_t4.pdf" target="view_window">TS-TFGridNet: Extending TF-GridNet for Label-Queried Target Sound Extraction via Embedding Concatenation</a>", in <i><strong>DCASE Challenge</strong></i>, technical report, 2025. [Rank 3rd place in <a href="https://dcase.community/challenge2025/task-spatial-semantic-segmentation-of-sound-scenes#results" target="view_window">DCASE2025 Challenge Task 4 - Spatial Semantic Segmentation of Sound Scenes</a>]

            </font></p>

			<h2><font color="#808080">Dissertation</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [1] <strong>Z.-Q. Wang</strong>, "<a href="publications/Zhong-Qiu_Wang_dissertation.pdf">Deep Learning Based Array Processing for Speech Separation, Localization, and Recognition</a>", <i>Ph.D. Dissertation</i>, The Ohio State University, Apr. 2020.

            </font></p>

			<h2><font color="#808080">Preprints</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [3] F. Zhao and <strong>Z.-Q. Wang</strong>, "<a href="https://arxiv.org/abs/2511.03244" target="view_window">Why Not Put a Microphone Near the Loudspeaker? A New Paradigm for Acoustic Echo Cancellation</a>", in <i>arxiv preprint arXiv:2511.03244</i>, 2025.

            <br>

            [2] K. Li, G. Chen, W. Sang, Y. Luo, Z. Chen, S. Wang, S. He, <strong>Z.-Q. Wang</strong>, A. Li, Z. Wu, and X. Hu, "<a href="https://arxiv.org/abs/2508.10830" target="view_window">Advances in Speech Separation: Techniques, Challenges, and Future Trends</a>", in <i>arxiv preprint arXiv:2508.10830</i>, 2025.

            <br>

            [1] <strong>Z.-Q. Wang</strong>, G. Wichern, and J. Le Roux, "<a href="https://arxiv.org/abs/2110.00570" target="view_window">Leveraging Low-Distortion Target Estimates for Improved Speech Enhancement</a>", in <i>arXiv preprint arXiv:2110.00570</i>, 2021. [<a href="https://github.com/espnet/espnet/blob/master/espnet2/enh/separator/ineube_separator.py" target="view_window"><u>Code</u></a>]

            </font></p>

			<h2><font color="#808080">Chalenge Rankings</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [3] 3rd place, DCASE2025 Challenge Task 4 - Spatial Semantic Segmentation of Sound Scenes, 2025.07

            <br>

            [2] 1st place, The 2nd Clarity Enhancement Challenge, 2022.12

            <br>

            [1] 1st place, The L3DAS22 3D Speech Enhancement Challenge, 2022.01

            </font></p>

			<h2><font color="#808080">Awards</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            [3] Selected as "World's Top 2% Scientists - Single-Year Impact" (入选全球前2%顶尖科学家年度榜单), 2025.09

            <br>

            [2] Graduate Research Award, Department of CSE at The Ohio State University, 2020.04

            <br>

            [1] Best Student Paper Award, IEEE ICASSP 2018.

            </font></p>

			<h2><font color="#808080">Professional Services</font></h2> 
			<p class="STYLE1" align="justify"><font style="font-size: 12pt">

            &#8226; Professional Membership

            <br>

            &emsp; &#9675; Committee Member, <a href="https://www.ccf.org.cn/Chapters/TC/TC_Listing/TCSDAP/">Speech Dialogue and Auditory Processing Technical Committee</a> (CCF TCSDAP, 语音对话与听觉专业委员会), China Computer Federation, 2024.8 - now

            <br>

            &emsp; &#9675; Committee Member, <a href="https://signalprocessingsociety.org/community-involvement/audio-and-acoustic-signal-processing/aasp-tc-home">Audio and Acoustic Signal Processing Technical Committee</a> (AASP-TC), IEEE Signal Processing Society, 2023.01 - 2025.12

            <br>

            &#8226; Journal Editorship

            <br> 

            &emsp; &#9675; Action Editor, Neural Networks, 2026.01 - 2028.12

            <br>

            &#8226; Conference Chair

            <br> 

            &emsp; &#9675; Area Chair, WASPAA, 2025

            <br>

            &emsp; &#9675; Area Chair, "Speech Coding and Enhancement", Interspeech 2024 and 2025

            <br>

            &emsp; &#9675; Area Chair, "Audio and Speech Source Separation", ICASSP 2024, 2025 and 2026

            <br>

            &emsp; &#9675; Challenge Organizer, "<a href="https://www.chimechallenge.org/current/task1/index">CHiME-7 Task 1: Distant automatic speech recognition with multiple devices in diverse scenarios</a>", CHiME workshop 2023

            <br>

            &emsp; &#9675; Special Session Chair, "Resource-efficient real-time neural speech separation", ICASSP 2023

            <br>

            &#8226; Meta-Reviewer

            <br>

            &emsp; &#9675; WASPAA 2023, ICASSP 2023

            <br>
 
            &#8226; Journal Reviewer

            <br>

            &emsp; &#9675; IEEE/ACM TASLP

            <br>

            &emsp; &#9675; Neural Networks

            <br>

            &emsp; &#9675; Speech Communication

            <br>

            &emsp; &#9675; Journal of The Acoustical Society of America

            <br>

            &emsp; &#9675; IEEE SPL

            <br>

            &emsp; &#9675; IEEE Open Journal of Signal Processing

            <br>

            &emsp; &#9675; Journal of Signal Processing Systems

            <br>

            &emsp; &#9675; EURASIP Journal on Audio, Speech, and Music Processing

            <br>

            &emsp; &#9675; Pattern Recognition Letters

            <br>

            &emsp; &#9675; Digital Signal Processing

            <br>

            &emsp; &#9675; IET Signal Processing

            <br>

            &emsp; &#9675; Electronics Letters

            <br>

            &#8226; Conference Reviewer

            <br>

            &emsp; &#9675; ICASSP, Interspeech, SLT, WASPAA, ASRU, CHiME workshop, IALP, IJCNN, NeurIPS

            </font></p>

		<h2><font color="#808080">Acknowledgements</font></h2>
		<a href="http://www.hit.edu.cn/" target="view_window"><img src="./images/hit.png" alt="" width="100" height="100"></a>
		<a href="https://www.osu.edu/" target="view_window"><img src="./images/osu.jpg" alt="" width="100" height="100"></a>
		<a href="https://www.cmu.edu/" target="view_window"><img src="./images/cmu.svg" alt="" width="100" height="100"></a>
		<a href="https://lti.cs.cmu.edu/" target="view_window"><img src="./images/lti.png" alt="" width="100" height="100"></a>
		<a href="https://www.sustech.edu.cn/en/" target="view_window"><img src="./images/sustech.png" alt="" width="100" height="100"></a>
		<a href="http://research.microsoft.com/en-us/" target="view_window"><img src="./images/msr.png" alt="" width="100" height="100"></a>
		<a href="http://www.merl.com/" target="view_window"><img src="./images/merl.jpg" alt="" width="234" height="100"></a>
		<a href="https://ai.google/" target="view_window"><img src="./images/google_ai.jpg" alt="" width="100" height="100"></a>
		<a href="https://www.osc.edu/" target="view_window"><img src="./images/osc.png" alt="" width="100" height="100"></a>
		<a href="https://www.ncsa.illinois.edu/" target="view_window"><img src="./images/ncsa.jpeg" alt="" width="100" height="100"></a>
        <br>
		<a href="https://www.psc.edu/" target="view_window"><img src="./images/PSC.jpeg" alt="" width="300" height="100"></a>
		<a href="https://about.meta.com/" target="view_window"><img src="./images/meta.png" alt="" width="100" height="100"></a>
		<a href="http://www.nsf.gov/" target="view_window"><img src="./images/nsf.png" alt="" width="100" height="100"></a>
		<a href="http://www.wpafb.af.mil/afrl/" target="view_window"><img src="./images/afrl.jpg" alt="" width="100" height="100"></a>
		<a href="https://www.huawei.com/cn/" target="view_window"><img src="./images/huawei.jpeg" alt="" width="100" height="100"></a>
		<a href="https://www.oppo.com/cn/" target="view_window"><img src="./images/oppo.jpg" alt="" width="300" height="100"></a>
		</div>
	</div>

	<div id="footer" class="row">
		<div class="col c12 aligncenter" >
		<p> <span>© Dec. 2025 by Zhong-Qiu Wang 
			<div style="display:inline-block;width:175px;">
                
			    <!--
				<script type="text/javascript" src="//rc.revolvermaps.com/0/0/7.js?i=27thib8064s&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;br=1&amp;sx=0&amp;ds=10" async="async">
				</script>
                --->
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=TA8pmEL4G-sfNd8cPpO5EpflMiKxnDDIwjKdS5HqpDw&cl=ffffff&w=a"></script>

			</div>
			</span>
			<span></span>
		</p>
		<div align="center">
		</div>
</p>
</div>
</div>
</div>
